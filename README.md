# ğŸ¤– Ollama AI Assistant

A modern, desktop AI chat application built with Python and CustomTkinter that interfaces with Ollama's local AI models.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Platform](https://img.shields.io/badge/platform-windows%20%7C%20macOS%20%7C%20linux-lightgrey.svg)

## âœ¨ Features

- ğŸ¨ **Modern Dark UI** - Beautiful interface built with CustomTkinter
- ğŸš€ **Local AI Integration** - Connect to Ollama's local AI models
- âš¡ **Real-time Chat** - Instant responses with threading support
- ğŸ¯ **Smart Text Handling** - Auto-sizing text boxes and smooth scrolling
- âŒ¨ï¸ **Keyboard Shortcuts** - Enter to send, Shift+Enter for new lines
- ğŸ”„ **Error Handling** - Robust connection and timeout error management
- ğŸ“± **Responsive Design** - Resizable interface with minimum size constraints
- ğŸ§¹ **Chat Management** - Clear chat history with one click

## ğŸ¬ Demo

The application provides a clean, intuitive interface for chatting with AI models:

- **Header**: Shows connection status and app title
- **Chat Area**: Scrollable conversation history with timestamps
- **Input Area**: Text input with Send and Clear buttons

## ğŸ“‹ Prerequisites

Before running the application, ensure you have:

1. **Python 3.8+** installed
2. **Ollama** installed and running locally
   - Download from: [ollama.ai](https://ollama.ai/)
   - Install a model: `ollama pull llama3.2:3b`
   - Start Ollama service: `ollama serve`

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/lordiod/ollama-ai-assistant.git
cd ollama-ai-assistant
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the Application
```bash
python main.py
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                          # ğŸšª Entry point
â”œâ”€â”€ chatbot_app.py                   # ğŸ—ï¸ Main application class
â”œâ”€â”€ requirements.txt                 # ğŸ“¦ Dependencies
â”œâ”€â”€ LICENSE                          # ğŸ“„ MIT License
â”œâ”€â”€ .gitignore                       # ğŸ™ˆ Git ignore rules
â”œâ”€â”€ models/                          # ğŸ“Š Data models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_message.py             # ğŸ’¬ Chat message model
â”œâ”€â”€ services/                        # ğŸ› ï¸ Business logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ollama_service.py           # ğŸ¤– Ollama API service
â”œâ”€â”€ ui/                             # ğŸ¨ User interface
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ header_component.py         # ğŸ“‹ Header with status
â”‚   â”œâ”€â”€ chat_display_component.py   # ğŸ’­ Chat display
â”‚   â””â”€â”€ input_component.py          # âŒ¨ï¸ Input handling
â””â”€â”€ utils/                          # ğŸ”§ Utilities
    â”œâ”€â”€ __init__.py
    â””â”€â”€ text_utils.py               # ğŸ“ Text processing
```

## ğŸ”§ Configuration

### Ollama Settings
The application uses these default settings (configurable in `services/ollama_service.py`):

- **Server URL**: `http://localhost:11434`
- **Default Model**: `llama3.2:3b`
- **Timeout**: 30 seconds

### UI Customization
Modify appearance in `chatbot_app.py`:

```python
ctk.set_appearance_mode("dark")  # "light", "dark", "system"
ctk.set_default_color_theme("blue")  # "blue", "green", "dark-blue"
```

## ğŸ› ï¸ Architecture

This project follows a **modular architecture** with clear separation of concerns:

### ğŸ“Š Models Layer
- **ChatMessage**: Handles message data with timestamps and serialization

### ğŸ› ï¸ Services Layer  
- **OllamaService**: Manages API communication, threading, and error handling

### ğŸ¨ UI Layer
- **HeaderComponent**: Status display and app branding
- **ChatDisplayComponent**: Message rendering and chat history
- **InputComponent**: User input and button interactions

### ğŸ”§ Utils Layer
- **text_utils**: Text processing and height calculations

## ğŸš¦ Error Handling

The application handles various error scenarios:

- **Connection Errors**: When Ollama service is unavailable
- **Timeout Errors**: When requests take too long
- **Model Errors**: When the specified model isn't available
- **Network Errors**: General connectivity issues

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.